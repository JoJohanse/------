Batch Normalization和Layer Normalization都是深度学习中常用的归一化方法，它们的主要目的是解决深度神经网络训练过程中的内部协变量偏移问题。下面是它们的主要区别：
1. **Batch Normalization（BN）**：BN是在每一层的每个特征维度上，对一个batch中的所有样本进行归一化。这意味着，如果batch size较小，或者数据分布不均匀，BN可能会导致模型性能下降。此外，BN依赖于batch size，因此在测试时需要使用移动平均来估计整个训练集的均值和方差。
小批次数据使用bn，波动较大
2. **Layer Normalization（LN）**：LN是在每个样本的所有特征维度上进行归一化。这意味着，LN不依赖于batch size，因此在训练和测试时的行为是一致的。然而，LN可能无法处理不同特征维度之间的依赖关系。

分词 将连续的文本序列切分成一个个独立的词汇单元。在英文中，分词相对简单，因为词汇之间通常由空格分隔。但在一些语言（如中文）中，分词就变得复杂许多，因为文本是连续的，没有明显的词汇分隔符。
1.基于规则
构建词表