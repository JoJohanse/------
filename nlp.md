Batch Normalization和Layer Normalization都是深度学习中常用的归一化方法，它们的主要目的是解决深度神经网络训练过程中的内部协变量偏移问题。下面是它们的主要区别：
1. **Batch Normalization（BN）**：BN是在每一层的每个特征维度上，对一个batch中的所有样本进行归一化。这意味着，如果batch size较小，或者数据分布不均匀，BN可能会导致模型性能下降。此外，BN依赖于batch size，因此在测试时需要使用移动平均来估计整个训练集的均值和方差。
小批次数据使用bn，波动较大
2. **Layer Normalization（LN）**：LN是在每个样本的所有特征维度上进行归一化。这意味着，LN不依赖于batch size，因此在训练和测试时的行为是一致的。然而，LN可能无法处理不同特征维度之间的依赖关系。

分词 将连续的文本序列切分成一个个独立的词汇单元。在英文中，分词相对简单，因为词汇之间通常由空格分隔。但在一些语言（如中文）中，分词就变得复杂许多，因为文本是连续的，没有明显的词汇分隔符。
1.基于规则
构建词表：将所有词汇放入词表中，然后对文本进行扫描，如果扫描到的词汇在词表中，则切分，否则继续扫描。这种方法的优点是简单易行，但缺点是需要构建词表，而且对于新词的处理比较困难
2.基于统计
两次同时出现越多，就越可能是一个词
对于新词比较灵活，缺点是需要大量的语料库
3.基于神经网络
lstm crf标注
LSTM：提取文本中的上下文信息和长期依赖关系
CRF：利用这些信息进行标注。

词嵌入：将词汇映射到实数向量空间中的方法。
word2vector：将词汇映射到一个低维的实数向量空间中，使得语义相近的词在向量空间中距离较近。
